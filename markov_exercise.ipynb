{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains (Chapter 03 Episode 4 Exercise)\n",
    "\n",
    "One major application of big data is to use data about the past to make predictions about the future; based on the patterns of what's happened before, you can guess what's likely to happen next. The more data you have, the more accurate your guesses can be. [Predictive analytics](https://en.wikipedia.org/wiki/Predictive_analytics) are used in every field that can gather a lot of data, like healthcare, professional sports, and economics. While this falls more often to data analysts than data engineers, it's still useful for data engineers to understand the ways in which data is consumed, so data can best be organized and cleaned for end use.\n",
    "\n",
    "A Markov chain is one kind of predictive model that guesses the next event based on the current event. Markov chains are used by cell phones, for instance, to guess which word you're going to type into a text next. Here's how it works: each time a word is encountered, the Markov chain keeps a tally of how often different words follow it. For example, let's say you sent the following four texts:\n",
    "- \"good morning\"\n",
    "- \"good gracious\"\n",
    "- \"good morning\"\n",
    "- \"good job\"\n",
    "\n",
    "We see that the word 'good' was followed by 'morning' twice, by 'gracious' once, and by 'job' once. How can we organize this information for use in a Markov chain? One way is with nested dictionaries, like this:\n",
    "```python\n",
    "{current_word1: {following_word1: num_of_times, following_word2: num_of_times, following_word3: num_of_times}}\n",
    "```\n",
    "...in our example, it would look like:\n",
    "```python\n",
    "{'good': {'morning': 2, 'gracious': 1, 'job':1}}\n",
    "```\n",
    "What if we then sent another text, reading \"good morning, sunshine\"? Now we have 'good' followed by 'morning' another time, and 'morning' followed by 'sunshine' once. So now our nested dictionary looks like this:\n",
    "```python\n",
    "{'good': {'morning': 3, 'gracious': 1, 'job':1}, 'morning':{'sunshine': 1}}\n",
    "```\n",
    "To summarize:\n",
    "- The outer dictionary contains each word from the input text as a key, followed by an inner dictionary as a value\n",
    "- Each inner dictionary contains as keys the words that can follow each key in the outer dictionary, with the number of times they actually follow that word as the value\n",
    "\n",
    "In this challenge, we'll take some Shakespeare sonnets as our input data, organize our data by the frequency with which one word follows any other, and then use those frequencies to generate a new Shakespeare-sounding text. It'll be gibberish, but could pass as something spoken by a person. Here's an example:\n",
    "\n",
    "\"Summer on to hideous winter, and she in thy husbandry? Or ten of thy glass, and tell the tyrants to breed another thee, or ten times refigured thee: then what could death do if thou some vial; treasure thou art, if ten times happier, be death’s conquest and she so fond will be the tillage of glass\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Step 1: Cleaning the Data\n",
    "\n",
    "- Check out your input data and make sure it has the kind of values you want (in this case, the words of the sonnets). If there's anything in there you don't want, write some code to clean it up. \n",
    "\n",
    "- The same word will be treated as two different words if there are differences in capitalization or punctuation. Decide if you want to keep it that way or to make the input data more uniform (for instance by stripping the punctuation and making all words lowercase.) You can do this before any other code is run, or do it within the dictionary-making code, to avoid looping over all the text twice.\n",
    "\n",
    "### Step 2: Creating A Nested Dictionary From Input Data\n",
    "\n",
    "- For the first stage of this challenge, use the `with open(<FILE_NAME>) as <FILE_OBJECT>` to open the input file `sonnets.txt`.\n",
    "\n",
    "- Go through each word of the input text and make it the key in a dictionary. For each of those keys, assign as a value a dictionary containing the words following it as keys, with frequency as values. It'll look something like this:\n",
    "```python\n",
    "{'': {'Those': 1}, 'Those': {'hours,': 1}, 'hours,': {'that': 1}, 'that': {'with': 3, 'unfair': 1, 'pay': 1, 'face': 1, 'my': 2, 'which': 1, 'thou': 3, 'bosom': 1, 'beauteous': 2, 'I': 1}, 'with': {'gentle': 3, 'frost,': 1, 'beauty': 1, 'winter': 1, 'thee.': 1, 'thyself': 1, 'thee,': 1, 'toil,': 1, 'travel': 1, 'sweets': 3, 'pleasure': 1, 'murderous': 1} ...\n",
    "# etc.\n",
    "```\n",
    "If you want to try this step on your own, go for it! If you'd like some more detailed instructions, see `input_to_nested_dict.md`.\n",
    "\n",
    "\n",
    "### Step 3: Weighted Probabilities\n",
    "\n",
    "We can see in our above example that the word \"good\" is three times more likely to be followed by \"morning\" than by \"gracious\" or \"job\". So if we're trying to create an output that mimics the pattern of the input data, we want \"morning\" to follow \"good\" three-fifths of the time, \"gracious\" to follow it one-fifth of the time, and \"job\" to follow it one-fifth of the time. We want the choice to be random, but weighted in favor of the more common outcomes.\n",
    "\n",
    "You can think of it as similar to making a program that predicts future weather patterns based on past weather pattern data. We know from experience that a warm, sunny day is most likely to be followed by another warm, sunny day, but it _could_ sometimes be followed by a cold, rainy day. There are predictable patterns, and within those patterns, randomness. This makes the output data more realistic, and keeps it from getting stuck in a single state.\n",
    "\n",
    "Python has a built-in [`random`](https://docs.python.org/3/library/random.html) module that can pick random numbers within a given range, or a random value from a sequence (like a list) of values. The Numpy library also has a [`random`](https://numpy.org/doc/stable/reference/random/index.html) module that includes some bonus functionality useful for more sophisticated math and science applications. Whichever you use, remember to import it at the top of the script.\n",
    "\n",
    "> Sidebar: Generating truly random sequences is harder than it might seen, and Python's `random()` actually generates pseudo-random numbers, whose pattern can be discerned. For cryptographically strong random numbers that would be safe for passwords, account authentication, or security tokens, use Python's [secrets module](https://docs.python.org/3/library/secrets.html)\n",
    "\n",
    "A few options for using the values in your Markov frequency dictionary to choose, using weighted probability, a key in that dictionary:\n",
    "\n",
    "1. Calculate what percent of the time a word appears in the frequency dictionary (its value divided by the sum of the values), and use these as the arguments for probability values in `random.choices()` or `numpy.random.choice()`\n",
    "\n",
    "1. Or, use this weighted probability algorithm outlined in the `weighted_prob_algorithm.md` file.\n",
    "\n",
    "1. Or, come up with your own solution! \n",
    "\n",
    "\n",
    "### Step 4: Writing the Output\n",
    "\n",
    "The hard part's done; now you just need to piece together to functions you wrote above so that each word chosen by your weighted probability function is concatenated to the word chosen before it. A few other things to consider for your program:\n",
    "\n",
    "1. The first word in your output will have to be selected at random, so there's somewhere to start.\n",
    "\n",
    "1. To clean up your output, try writing a function that capitalizes a word if it's the first word a sentence, and makes other words lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For unprovident live my mutual eyes her prime so fond with beauty and being unused the bounteous of his annoy they do see whose speechless song being frank seeming one.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open(\"./data/sonnets.txt\", \"r\") as myfile:\n",
    "    text = \"\"\n",
    "    for line in myfile:\n",
    "        line = line.rstrip()\n",
    "        text = text + \" \" + line\n",
    "\n",
    "#text = \"Summer on to hideous winter, and she in thy husbandry? Or ten of thy glass, and tell the tyrants to breed another thee, or ten times refigured thee: then what could death do if thou some vial; treasure thou art, if ten times happier, be death’s conquest and she so fond will be the tillage of glass\"\n",
    "\n",
    "# Step 1: Cleaning the Data\n",
    "clean_text = text.replace(\",\", \"\").replace(\"?\", \"\").replace(\":\", \"\").replace(\";\", \"\").replace(\".\", \"\") # clean_text = text.replace(\"'s\", \"\")\n",
    "\n",
    "text_list = list(clean_text.lower().split(\" \"))\n",
    "text_list.remove(\"\") # removes space between different sections of sonnets\n",
    "\n",
    "# Step 2: Creating A Nested Dictionary From Input Data\n",
    "\n",
    "def create_dict(text_list):\n",
    "    text_dict = {}\n",
    "    text_set = set(text_list) # remove duplicate words\n",
    "    for word in text_set:\n",
    "        text_dict[word] = []\n",
    "    return text_dict\n",
    "\n",
    "text_dict = create_dict(text_list)\n",
    "\n",
    "def next_word_identifier(text_list, text_dict):\n",
    "    text_dict_with_next = text_dict\n",
    "    \n",
    "    for key in text_dict:\n",
    "        for i in range(len(text_list)-1):\n",
    "            if key == text_list[i]:\n",
    "                text_dict[key].append(text_list[i+1])\n",
    "    return text_dict_with_next\n",
    "\n",
    "next_word_identifier(text_list, text_dict)\n",
    "\n",
    "def create_dict_of_dict(text_dict):\n",
    "    final_dict = {}\n",
    "\n",
    "    for key, word_list in text_dict.items():\n",
    "        count_dict = {}\n",
    "        for word in word_list:\n",
    "            final_dict[key] = count_dict\n",
    "            count_dict[word] = word_list.count(word)\n",
    "    #print(final_dict)\n",
    "    return final_dict\n",
    "\n",
    "final_dict = create_dict_of_dict(text_dict)\n",
    "\n",
    "\n",
    "#user_input = input(\"Input a word from the text for recommended next word: \").lower()\n",
    "\n",
    "def prob_calc(first_word, final_dict):\n",
    "    key_list = []\n",
    "    prop_list = []\n",
    "    recom_next_word = \"\"\n",
    "    for key, value in final_dict.items():\n",
    "        if key == first_word:\n",
    "            key_list = list(final_dict[key].keys())\n",
    "            prop_list = list(final_dict[key].values())\n",
    "            recom_next_word = random.choices(key_list, weights=prop_list)\n",
    "    return recom_next_word[0]\n",
    "\n",
    "first_word = random.choices(text_list)[0]\n",
    "next_word = prob_calc(first_word, final_dict)\n",
    "paragraph = \"\"\n",
    "\n",
    "for word in range(28):\n",
    "    next_word = prob_calc(next_word, final_dict)\n",
    "    paragraph += f\" {prob_calc(next_word, final_dict)}\"\n",
    "\n",
    "print(first_word.capitalize() + \" \" + second_word + paragraph + \".\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91c15be82eb4901e4ccdc4792f9df8a6a134fbef1a8d28e31a106ce8c84fcb23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
